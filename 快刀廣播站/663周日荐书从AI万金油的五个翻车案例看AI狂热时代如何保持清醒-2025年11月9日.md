# 663｜周日荐书：从《AI万金油》的五个翻车案例，看AI狂热时代如何保持清醒

**快刀广播站** 
 **2025年11月9日** 

![Article Cover]( https://piccdn3.umiwi.com/img/202511/08/202511081204497989099789.jpeg )

你好，我是快刀青衣。欢迎收听快刀广播站，每天带你看AI。

咱们又到了周日荐书环节。今天要推荐的书，我断断续续看了半个月才看完。倒不是因为内容难，而是这本书的每个章节之间没有太强的上下文关联，所以不用逼着自己快点儿读完。这本书叫《AI万金油：商业幻想与科技狂潮》，它其实没怎么讲AI的神奇功能，恰恰相反， **这本书从头到尾都在“泼冷水”——告诉你AI在哪些地方翻车了，翻得有多惨。**

先说说这个书名。“万金油”对应的英文是“snake oil”，也就是蛇油。这个词来自19世纪末20世纪初的美国，当时江湖游医到处兜售一种号称“包治百病”的蛇油药水，小到头痛、大到癌症，什么都能治。但实际上呢？啥用没有，纯粹是骗钱的。我也在文稿里贴了一张当年的那种复古广告，你可以看一下。

![Image]( https://piccdn2.umiwi.com/uploader/image/ddarticle/2025110821/1892585132200609628/110821.png )

其实如果同学里有像我一样的80后的话，估计对这类产品一点儿都不陌生，例如当年那个风靡一时的505神功元气袋，现在想想，真是对智商的极大侮辱。作者用“万金油”这个词来形容当下的AI热潮，意思很明确： **很多被吹得神乎其神的AI技术，实际效果跟宣传相去甚远，甚至潜藏风险。**

这本书有两位作者。阿尔文德·纳拉亚南是普林斯顿大学计算机科学教授，同时也是一位信息技术政策研究者；另一位作者萨亚什·卡普尔是普林斯顿大学计算机博士，曾在Facebook（现Meta）担任软件工程师。两人的研究方向各有侧重，一个偏重理论，一个偏重工程实践，形成了完美搭配。他们都入选了《时代》周刊“全球AI领域最具影响力100人”。更有意思的是，这本书源自两人2020年在普林斯顿大学开设的一门课程，名字叫“预测的局限”。他们花了几年时间，收集了大约50个AI应用的真实案例，涵盖医疗、金融、保险、教育等各个领域，系统性地告诉你： **AI没你想的那么神。**

最让我震撼的是书里提到的一个案例：美国联合健康集团要求员工必须服从AI的决策，如果多次反对AI的判断，甚至可能被解雇。听起来很科幻对吧？但事后调查发现，这个AI系统做出的决策中，超过90%都是错误的。你没看错，就是90%。

所以今天，我就从这本书里挑出5个最让我警醒的AI翻车案例，和你详细聊聊。不是为了制造焦虑，也不是让你不要用AI，而是希望咱们在使用AI的时候，能多一分审慎，少一分盲目。

当然，这里必须先说明一下， **这本书研究的AI，大多是预测型AI，而不是我们现在常用的生成型AI。** 什么是预测型AI？其实就是基于过往数据，推测“已存在但未明确的答案”，比如天气预报就是很典型的例子。而生成型AI，就是帮你创造“此前没有的新内容”，比如你说“今天要下雨，把我的照片改成撑着雨伞在花园里漫步的样子”，它就能完成这样的创作。

咱们先接着说前面提到的第一个“翻车”案例。美国联合健康集团，这家大型健康保险公司曾部署过一套AI系统，并且明确要求员工必须服从这套AI系统的决策，如果多次反对可能被解雇。想象一下：一位拥有十几年经验的医疗审核专员，看到AI的决策明显不合理，却不敢提出异议。AI说不批准住院，就只能不批准；AI说拒绝报销，就只能拒绝。 **人的专业判断，在AI面前一文不值。**

结果呢？事后调查发现，这个AI系统做出的决策中，超过90%都是错误的——也就是十个决策里九个都有问题。说实话，这让我想到了“狗仗人势”这个词，只不过这次换成了 **“人仗AI势”** 。公司分明是把AI当成了推卸责任的挡箭牌：反正是AI定的，不是我定的。

这个案例最可怕的地方， **不是AI会出错，而是用AI替代人的专业判断和责任承担。**

第二个翻车案例来自好事达保险公司， **聚焦AI如何“精准割韭菜”。**

2013年，好事达用AI调整汽车保险费率，他们给AI设定的目标很明确：在不流失过多客户的前提下，最大化利润。说白了就是找出对价格不敏感的客户，使劲薅羊毛。AI被输入大量客户数据，比如客户的年龄、住址、驾驶记录、信用评分等，然后输出了一份“可榨取客户名单”。结果，这份名单里62岁以上老年人的比例异常高。

这背后的原因很简单：AI发现老年人更少货比三家，更换保险公司的意愿也更低。于是这个群体被系统性地给予更少折扣和更高的保费涨幅。保险公司从没明确说“要歧视老年人”，但“利润最大化”的目标叠加数据训练，必然导向这样的结果。

老实说， **这就是“算法黑箱”的典型表现。** 当企业声称“这是AI的客观决策”时，你得追问：AI的目标函数到底是什么？是为了公平，还是为了利润？绝大多数公司不会公开这些核心信息，而这种不透明性，恰恰是最危险的地方。书里的这个案例已经是十年前的事，但如今依靠AI做个性化判断，早已成了很多电商、保险应用的常规操作，可黑箱规则里，还藏着多少我们不知道的“魔鬼细节”？

顺着这个案例，书里还提到了另一件事。2022年，纽约著名体育和音乐会场馆麦迪逊广场花园，发生了一起争议事件：律师尼科莱特·兰迪因被拒绝入场观看玛丽亚·凯莉的音乐会而备受关注。

要知道，兰迪的男朋友为庆祝她的生日购买了价值近400美元的门票，而她被拒的原因着实离谱。场馆的入场系统里有一条规则，就是对于那些起诉他们场馆的律师事务所，禁止这些单位的所有律师入场，无论这些律师是否直接参与对场馆的诉讼。哪怕是持有场馆长期季票的忠实球迷，也不例外。没人知道类似这样的黑箱里，还藏着多少奇葩规则，供AI使用。

第三个翻车案例发生在新冠疫情期间，揭示了AI的一个致命问题： **走捷径。**

疫情爆发后，不少医学研究声称AI能通过胸部X光片，高精度区分新冠患者与普通肺炎患者。但后续有人系统审查400多篇相关论文后发现，这些研究存在严重方法论缺陷，几乎不具备临床应用价值。

**核心问题出在数据上** 。很多研究的训练数据存在明显偏差：患病影像几乎全部来自成人，未患病的影像却全来自儿童。AI并没有学会识别新冠病变特征， **只是精准捕捉到了一个统计规律** ：成人更可能患新冠，儿童可能只是普通感冒。

所以AI并不是在识别磨玻璃影、白肺这些真正的新冠特征，而是在判断：这是成人的片子还是儿童的？这个逻辑让AI在原数据集上准确率极高，但逻辑完全错误。

说实话，这让我想起一个段子：以前有老师拿一批照片，让学生区分狼和狗，结果学生发现，照片里有雪的就是狼，没雪的就是狗。AI的问题就在这：它太“聪明”了，会找各种捷径，但不懂什么叫真正的因果关系。

前面三个案例里，AI主要是不靠谱；但第四个案例， **是AI被用来纯粹作恶——AI成了诈骗工具** 。这个案例其实在作者给大学开课的时候，还属于比较少见的。可放到现在，这类诈骗可是太常见了，而且技术门槛低到离谱，普通人稍微研究一下，就能做出以假乱真的音视频。

书里提到了2019年的一个案例：一名英国公司高管接到一通电话，那头是德国母公司CEO的声音——口音、语调都完美复刻。CEO说有紧急业务，要求他立即汇款22万欧元。高管没起疑心，把钱打了过去。结果那根本不是CEO本人，而是AI伪造的声音。

在2019年的时候，这类诈骗还需要专业技术和不菲成本。但现在，只要有几秒钟的真实录音片段，任何人用开源工具就能生成以假乱真的语音，语音诈骗的门槛被前所未有地拉低了。而且如今被伪造的不只是语音，视频也能轻松实现。你在网上看到的名人言论、朋友发来的视频，都可能是AI生成的，而普通人很难分辨真假。

所以对咱们同学来说，最实用的建议就是：凡 **是涉及钱的事，一定要多方核实，不要轻信任何语音或视频，哪怕听起来再真实。**

最后一个案例，直接揭示了 **AI在社会预测领域的根本局限** 。AI不是万能的，而且还有很多值得我们深思的问题。

2015年，普林斯顿大学开展了一项研究，追踪了4000多名儿童，从他们出生起就进行全方位调查，一直持续到15岁。研究者随后组织了一场预测竞赛，向全球参赛者开放这些孩子从出生到9岁的所有数据，涵盖约1万个特征维度。竞赛要求参赛者用AI模型，预测这些孩子15岁时的表现，包括GPA成绩、住房稳定性等6项关键指标。

全球共有160支顶尖团队参赛。结果呢？ **没有任何模型表现出色，即使最好的模型也仅比随机猜测略强。**

为什么？研究人员拜访预测误差最大的家庭发现：一个原本成绩较差的孩子突然表现出色，背后原因就是一名热心邻居的关键支持，给他辅导作业、提供食物等。但这些来自家庭外的支持信息，数据里根本没有记录。

这就是AI在社会预测领域的困境： **数据维度的天然缺失。** 要实现精准预测，可能需要记录孩子的一举一动甚至每个念头，但这在伦理、法律和实践层面都不可接受。而且很多涉及人的预测问题，规律本身就容易变化。

所以面对那些声称能“预测员工离职率”、“预测学生辍学风险”的AI产品，你一定要多加注意，你可以去了解，但千万别用这类工具轻易给一个真实的人贴标签。

**人不是冰冷的图片，社会也不是规整的数据集，有些东西，AI真的预测不了** 。至于那些所谓的“AI玄学”，我的态度就是当成一个小游戏玩玩就行，可别当真。

看完这5个案例，你可能会问：刀哥，既然AI这么容易翻车，我们还要不要用AI？

当然要用，但关键是 **要保持清醒，保持审慎** 。这本《AI万金油》最大的价值，不是告诉你AI有多糟糕，而是告诉你 **AI的边界在哪里** 。它让你明白，AI不是万能的，更不是中立的。它会出错，会走捷径，会被人利用来谋取私利。

对咱们普通人来说，记住三点就够了：

第一， **不要盲目相信AI的判断** **。** 即使是大公司的AI系统，错误率也可能高得离谱。

第二， **警惕** **“** **算法黑箱** **”。** 当企业说“这是AI的客观决策”时，要追问目标是什么，或者我们站在AI的角度去思考可能存在的问题，这样至少提高了我们在AI时代的思考能力。

第三， **提高对AI生成内容的辨别能力** **。** 涉及钱和重要决策的事，一定要多方核实。

好，今天广播就到这里。如果你觉得有帮助，欢迎分享转发给你的朋友。明天咱们接着聊AI。

![Image]( https://piccdn2.umiwi.com/uploader/image/ddarticle/2025110821/1892585162265930320/110821.jpeg )

---

![Image]( https://piccdn2.umiwi.com/uploader/image/ddarticle/2025110821/1892585182666500688/110821.jpeg )